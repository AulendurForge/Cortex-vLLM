# Cortex Docker Image Versions
# =============================
# This file is the single source of truth for Docker image versions.
# All scripts should source this file to ensure version consistency.
#
# To update versions:
# 1. Update the versions here
# 2. Run: make prepare-offline (on internet-connected machine)
# 3. Transfer the offline package
# 4. Run: make load-offline && make verify-offline
#
# NOTE: Environment variables can still override these defaults.

# Inference Engines
# -----------------
# vLLM: High-performance inference engine for transformer models
# - Use "latest" for the newest features
# - Use a specific tag (e.g., "v0.8.0") for reproducibility in production
# - Qwen3 models require vLLM with Transformers >= 4.51
CORTEX_VLLM_VERSION="${CORTEX_VLLM_VERSION:-latest}"

# llama.cpp: Lightweight inference engine for GGUF models
# - "server-cuda" includes CUDA support
# - Other tags: "server" (CPU only), "full-cuda" (with tools)
CORTEX_LLAMACPP_TAG="${CORTEX_LLAMACPP_TAG:-server-cuda}"

# Infrastructure
# --------------
# These should rarely need updating unless specific features are needed
CORTEX_PYTHON_VERSION="${CORTEX_PYTHON_VERSION:-3.11-slim}"
CORTEX_NODE_VERSION="${CORTEX_NODE_VERSION:-18-alpine}"
CORTEX_POSTGRES_VERSION="${CORTEX_POSTGRES_VERSION:-16}"
CORTEX_REDIS_VERSION="${CORTEX_REDIS_VERSION:-7}"

# Monitoring
# ----------
CORTEX_PROMETHEUS_VERSION="${CORTEX_PROMETHEUS_VERSION:-v2.47.0}"
CORTEX_NODE_EXPORTER_VERSION="${CORTEX_NODE_EXPORTER_VERSION:-v1.6.1}"
CORTEX_DCGM_EXPORTER_VERSION="${CORTEX_DCGM_EXPORTER_VERSION:-3.1.8-3.1.5-ubuntu22.04}"
CORTEX_CADVISOR_VERSION="${CORTEX_CADVISOR_VERSION:-v0.47.0}"

# Full image references (computed from versions above)
CORTEX_VLLM_IMAGE="vllm/vllm-openai:${CORTEX_VLLM_VERSION}"
CORTEX_LLAMACPP_IMAGE="ghcr.io/ggml-org/llama.cpp:${CORTEX_LLAMACPP_TAG}"


