'use client';

import Link from 'next/link';
import { Card, SectionTitle, InfoBox, Badge, Button } from '../../../../../src/components/UI';
import { cn } from '../../../../../src/lib/cn';

export default function AddingModels() {
  return (
    <section className="space-y-6">
      {/* Introduction */}
      <Card className="p-5 bg-gradient-to-r from-emerald-500/5 via-purple-500/5 to-cyan-500/5 border-white/5">
        <p className="text-[13px] text-white/80 leading-relaxed">
          Adding a model to Cortex involves a guided workflow that walks you through engine selection, 
          model location, and configuration. This guide covers both <strong className="text-cyan-300">Online</strong> mode 
          (downloading from HuggingFace) and <strong className="text-purple-300">Offline</strong> mode 
          (using pre-downloaded local files).
        </p>
      </Card>

      {/* Add Model Button Location */}
      <section className="space-y-3">
        <SectionTitle variant="cyan" className="text-[10px]">Starting the Workflow</SectionTitle>
        <Card className="p-4 bg-white/[0.02] border-white/5 space-y-4">
          <p className="text-[12px] text-white/70 leading-relaxed">
            Navigate to <strong className="text-white">Models</strong> in the sidebar, then click the 
            <span className="inline-flex items-center mx-1.5 px-2 py-0.5 bg-cyan-500/20 text-cyan-300 rounded text-[10px] font-bold">
              ‚ûï Add Model
            </span>
            button in the top right. This opens a step-by-step wizard that guides you through configuration.
          </p>
          <div className="flex flex-wrap gap-2">
            <Badge className="bg-white/10 text-white/60 border-white/10 text-[9px]">Step 1: Engine & Mode</Badge>
            <Badge className="bg-white/10 text-white/60 border-white/10 text-[9px]">Step 2: Model Selection</Badge>
            <Badge className="bg-white/10 text-white/60 border-white/10 text-[9px]">Step 3: Core Settings</Badge>
            <Badge className="bg-white/10 text-white/60 border-white/10 text-[9px]">Step 4: Startup Config</Badge>
            <Badge className="bg-white/10 text-white/60 border-white/10 text-[9px]">Step 5: Request Defaults</Badge>
            <Badge className="bg-white/10 text-white/60 border-white/10 text-[9px]">Step 6: Summary & Launch</Badge>
          </div>
        </Card>
      </section>

      {/* Online vs Offline Modes */}
      <section className="space-y-3">
        <SectionTitle variant="purple" className="text-[10px]">Online vs Offline Mode</SectionTitle>
        <div className="grid grid-cols-1 lg:grid-cols-2 gap-4">
          {/* Online Mode */}
          <Card className="p-5 bg-cyan-500/5 border-cyan-500/20 space-y-4">
            <div className="flex items-center gap-3">
              <div className="text-2xl">üåê</div>
              <div>
                <h3 className="text-[13px] font-bold text-cyan-300 uppercase tracking-wider">Online Mode</h3>
                <p className="text-[10px] text-white/50">Download from HuggingFace Hub</p>
              </div>
            </div>

            <div className="space-y-2">
              <div className="text-[10px] font-bold text-white/50 uppercase tracking-wider">How it works</div>
              <p className="text-[11px] text-white/70 leading-relaxed">
                Provide a HuggingFace repository ID (e.g., <code className="text-cyan-300 bg-black/30 px-1 rounded">meta-llama/Llama-3.1-8B-Instruct</code>). 
                When you start the model, Cortex downloads the weights to its cache and loads them.
              </p>
            </div>

            <div className="space-y-2">
              <div className="text-[10px] font-bold text-white/50 uppercase tracking-wider">Required fields</div>
              <ul className="text-[11px] text-white/70 space-y-1.5">
                <li className="flex items-start gap-2">
                  <span className="text-cyan-400">‚Ä¢</span>
                  <span><strong>Repo ID:</strong> owner/model-name format</span>
                </li>
                <li className="flex items-start gap-2">
                  <span className="text-cyan-400">‚Ä¢</span>
                  <span><strong>HF Token:</strong> Required for gated models (Llama, Mistral, etc.)</span>
                </li>
              </ul>
            </div>

            <div className="p-3 bg-black/30 rounded-lg border border-cyan-500/20 space-y-2">
              <div className="text-[10px] font-bold text-cyan-300">Example Repo IDs</div>
              <div className="text-[10px] text-white/60 space-y-1 font-mono">
                <div>meta-llama/Llama-3.1-8B-Instruct</div>
                <div>mistralai/Mistral-7B-Instruct-v0.3</div>
                <div>Qwen/Qwen2.5-7B-Instruct</div>
                <div>nomic-ai/nomic-embed-text-v1.5</div>
              </div>
            </div>

            <InfoBox variant="blue" className="text-[10px] p-2.5">
              <strong>Gated Models:</strong> Many popular models require accepting terms on HuggingFace. 
              Visit the model page, accept the license, then use an HF token with read permissions.
            </InfoBox>
          </Card>

          {/* Offline Mode */}
          <Card className="p-5 bg-purple-500/5 border-purple-500/20 space-y-4">
            <div className="flex items-center gap-3">
              <div className="text-2xl">üìÅ</div>
              <div>
                <h3 className="text-[13px] font-bold text-purple-300 uppercase tracking-wider">Offline Mode</h3>
                <p className="text-[10px] text-white/50">Use local model files</p>
              </div>
            </div>

            <div className="space-y-2">
              <div className="text-[10px] font-bold text-white/50 uppercase tracking-wider">How it works</div>
              <p className="text-[11px] text-white/70 leading-relaxed">
                Point to a folder containing model files on your local disk. Essential for air-gapped 
                environments, GGUF models, or when you've pre-downloaded models for faster startup.
              </p>
            </div>

            <div className="space-y-2">
              <div className="text-[10px] font-bold text-white/50 uppercase tracking-wider">Setup steps</div>
              <ol className="text-[11px] text-white/70 space-y-1.5">
                <StepItem num={1}>Set your <strong>Base Directory</strong> (e.g., /models)</StepItem>
                <StepItem num={2}>Place model folders inside this directory</StepItem>
                <StepItem num={3}>Click <strong>Refresh</strong> to scan available folders</StepItem>
                <StepItem num={4}>Select a folder‚ÄîCortex inspects its contents</StepItem>
              </ol>
            </div>

            <div className="p-3 bg-black/30 rounded-lg border border-purple-500/20 space-y-2">
              <div className="text-[10px] font-bold text-purple-300">Folder Structure Example</div>
              <pre className="text-[9px] text-white/60 font-mono">
{`/models/
‚îú‚îÄ‚îÄ Llama-3.1-8B-Instruct/
‚îÇ   ‚îú‚îÄ‚îÄ config.json
‚îÇ   ‚îú‚îÄ‚îÄ model.safetensors
‚îÇ   ‚îî‚îÄ‚îÄ tokenizer.json
‚îú‚îÄ‚îÄ GPT-OSS-120B-GGUF/
‚îÇ   ‚îú‚îÄ‚îÄ model-Q4_K_M.gguf
‚îÇ   ‚îî‚îÄ‚îÄ tokenizer.json
‚îî‚îÄ‚îÄ Mistral-7B-split/
    ‚îú‚îÄ‚îÄ model-00001-of-00003.gguf
    ‚îú‚îÄ‚îÄ model-00002-of-00003.gguf
    ‚îî‚îÄ‚îÄ model-00003-of-00003.gguf`}
              </pre>
            </div>

            <InfoBox variant="purple" className="text-[10px] p-2.5">
              <strong>GGUF Models:</strong> When you select a folder with GGUF files, Cortex shows a 
              file picker with quantization levels. Choose your preferred variant (Q4_K_M, Q8_0, etc.).
            </InfoBox>
          </Card>
        </div>
      </section>

      {/* Step-by-Step: Online vLLM */}
      <section className="space-y-3">
        <SectionTitle variant="blue" className="text-[10px]">Walkthrough: Online Mode with vLLM</SectionTitle>
        <Card className="p-4 bg-white/[0.02] border-white/5 space-y-4">
          <p className="text-[12px] text-white/70 leading-relaxed">
            The most common deployment: downloading a model from HuggingFace and running it with vLLM.
          </p>

          <div className="space-y-4">
            <WalkthroughStep 
              step={1} 
              title="Engine & Mode Selection" 
              color="blue"
            >
              <p className="text-[11px] text-white/70 leading-relaxed mb-2">
                Select <strong className="text-blue-300">vLLM</strong> as your engine and <strong className="text-cyan-300">Online</strong> as your mode.
              </p>
              <div className="flex gap-2">
                <Badge className="bg-blue-500/20 text-blue-300 border-blue-500/30 text-[9px]">vLLM Selected</Badge>
                <Badge className="bg-cyan-500/20 text-cyan-300 border-cyan-500/30 text-[9px]">Online Mode</Badge>
              </div>
            </WalkthroughStep>

            <WalkthroughStep 
              step={2} 
              title="Model Selection" 
              color="emerald"
            >
              <p className="text-[11px] text-white/70 leading-relaxed mb-2">
                Enter the HuggingFace repository ID. For gated models, also provide your HuggingFace token.
              </p>
              <div className="p-2 bg-black/30 rounded border border-white/10">
                <div className="text-[10px] text-white/50 mb-1">Repo ID</div>
                <code className="text-[11px] text-emerald-300">meta-llama/Llama-3.1-8B-Instruct</code>
              </div>
            </WalkthroughStep>

            <WalkthroughStep 
              step={3} 
              title="Core Settings" 
              color="amber"
            >
              <p className="text-[11px] text-white/70 leading-relaxed mb-2">
                Configure model identity and resource allocation:
              </p>
              <ul className="text-[11px] text-white/70 space-y-1.5 ml-4">
                <li><strong>Name:</strong> Friendly display name (e.g., "Llama 3.1 8B")</li>
                <li><strong>Served Model Name:</strong> API identifier (e.g., "llama-3.1-8b")</li>
                <li><strong>Task:</strong> generate (chat) or embed (embeddings)</li>
                <li><strong>GPUs:</strong> Select which GPUs to use</li>
                <li><strong>Max Context:</strong> Token limit (8192-131072)</li>
                <li><strong>Memory Utilization:</strong> How much VRAM to use (0.8-0.95)</li>
              </ul>
            </WalkthroughStep>

            <WalkthroughStep 
              step={4} 
              title="Startup Configuration" 
              color="red"
            >
              <p className="text-[11px] text-white/70 leading-relaxed mb-2">
                <em>Optional:</em> Add custom command-line arguments or environment variables for the inference engine.
                Most users can skip this step‚Äîit's for advanced tuning.
              </p>
            </WalkthroughStep>

            <WalkthroughStep 
              step={5} 
              title="Request Defaults" 
              color="purple"
            >
              <p className="text-[11px] text-white/70 leading-relaxed mb-2">
                <em>Optional:</em> Set default sampling parameters (temperature, top_p, repetition penalty) 
                that apply when clients don't specify them. Safe to skip for most deployments.
              </p>
            </WalkthroughStep>

            <WalkthroughStep 
              step={6} 
              title="Summary & Launch" 
              color="cyan"
            >
              <p className="text-[11px] text-white/70 leading-relaxed mb-2">
                Review your configuration summary. Click <strong className="text-cyan-300">Launch Model</strong> to create 
                the configuration. Then from the Models table, click <strong className="text-emerald-300">Start</strong> to deploy.
              </p>
            </WalkthroughStep>
          </div>
        </Card>
      </section>

      {/* Step-by-Step: Offline llama.cpp */}
      <section className="space-y-3">
        <SectionTitle variant="emerald" className="text-[10px]">Walkthrough: Offline Mode with llama.cpp (GGUF)</SectionTitle>
        <Card className="p-4 bg-white/[0.02] border-white/5 space-y-4">
          <p className="text-[12px] text-white/70 leading-relaxed">
            Running a GGUF model with llama.cpp‚Äîrequired for GPT-OSS and multi-part GGUF files.
          </p>

          <div className="space-y-4">
            <WalkthroughStep 
              step={1} 
              title="Engine & Mode Selection" 
              color="emerald"
            >
              <p className="text-[11px] text-white/70 leading-relaxed mb-2">
                Select <strong className="text-emerald-300">llama.cpp</strong>. Mode automatically switches to 
                <strong className="text-purple-300"> Offline</strong> (llama.cpp requires local files).
              </p>
              <div className="flex gap-2">
                <Badge className="bg-emerald-500/20 text-emerald-300 border-emerald-500/30 text-[9px]">llama.cpp Selected</Badge>
                <Badge className="bg-purple-500/20 text-purple-300 border-purple-500/30 text-[9px]">Offline Mode</Badge>
              </div>
            </WalkthroughStep>

            <WalkthroughStep 
              step={2} 
              title="Model Selection" 
              color="purple"
            >
              <div className="space-y-3">
                <p className="text-[11px] text-white/70 leading-relaxed">
                  Set your base directory and select a folder containing GGUF files:
                </p>
                <ol className="text-[11px] text-white/70 space-y-1.5 ml-4">
                  <li>1. Enter base path: <code className="text-purple-300 bg-black/30 px-1 rounded">/models</code></li>
                  <li>2. Click <strong>Save & Scan</strong></li>
                  <li>3. Select your model folder from the dropdown</li>
                  <li>4. Cortex inspects the folder and shows available GGUF files</li>
                </ol>

                <div className="p-3 bg-purple-500/10 border border-purple-500/20 rounded-lg space-y-2">
                  <div className="text-[10px] font-bold text-purple-300">GGUF Selection</div>
                  <p className="text-[10px] text-white/60">
                    When multiple quantizations exist (Q4_K_M, Q5_K_M, Q8_0), Cortex groups them and 
                    recommends the best option. Multi-part files are detected automatically.
                  </p>
                </div>

                <div className="p-3 bg-amber-500/10 border border-amber-500/20 rounded-lg space-y-2">
                  <div className="text-[10px] font-bold text-amber-300">Tokenizer Requirement</div>
                  <p className="text-[10px] text-white/60">
                    GGUF files need a tokenizer. Either provide an HF repo ID (e.g., "meta-llama/Llama-3.1-8B") 
                    or ensure tokenizer.json exists in the folder.
                  </p>
                </div>
              </div>
            </WalkthroughStep>

            <WalkthroughStep 
              step={3} 
              title="Core Settings (llama.cpp specific)" 
              color="amber"
            >
              <p className="text-[11px] text-white/70 leading-relaxed mb-2">
                Configure llama.cpp-specific parameters:
              </p>
              <ul className="text-[11px] text-white/70 space-y-1.5 ml-4">
                <li><strong>Context Size:</strong> Total context window (e.g., 16384)</li>
                <li><strong>Parallel Slots:</strong> Concurrent request capacity (e.g., 16)</li>
                <li><strong>GPU Layers (ngl):</strong> Layers to offload to GPU (999 = all)</li>
                <li><strong>GPU Selection:</strong> Which GPUs to use (creates tensor_split)</li>
                <li><strong>KV Cache Type:</strong> q8_0 recommended for balance</li>
              </ul>
              <div className="mt-2 p-2 bg-blue-500/10 border border-blue-500/20 rounded text-[10px] text-white/60">
                <strong className="text-blue-300">Context per slot:</strong> Total context √∑ parallel slots. 
                For 16384 context with 16 slots = 1024 tokens per slot.
              </div>
            </WalkthroughStep>

            <WalkthroughStep 
              step={4} 
              title="Advanced: Speculative Decoding" 
              color="purple"
            >
              <p className="text-[11px] text-white/70 leading-relaxed mb-2">
                <em>Optional but powerful:</em> Use a small draft model to accelerate inference.
                Point to a smaller GGUF file (e.g., 0.5B draft model) to speed up your main model.
              </p>
            </WalkthroughStep>
          </div>
        </Card>
      </section>

      {/* Tips and Common Issues */}
      <section className="space-y-3">
        <SectionTitle variant="amber" className="text-[10px]">Tips for Success</SectionTitle>
        <div className="grid grid-cols-1 lg:grid-cols-2 gap-4">
          <Card className="p-4 bg-emerald-500/5 border-emerald-500/20 space-y-3">
            <div className="text-[10px] font-bold text-emerald-300 uppercase tracking-wider">‚úì Best Practices</div>
            <ul className="text-[11px] text-white/70 space-y-2">
              <li className="flex items-start gap-2">
                <span className="text-emerald-400">‚úì</span>
                <span>Start with conservative settings (lower context, fewer slots) and scale up</span>
              </li>
              <li className="flex items-start gap-2">
                <span className="text-emerald-400">‚úì</span>
                <span>Use the <strong>Calculator</strong> button to estimate VRAM before deployment</span>
              </li>
              <li className="flex items-start gap-2">
                <span className="text-emerald-400">‚úì</span>
                <span>Run a <strong>Test</strong> after starting to verify the model responds correctly</span>
              </li>
              <li className="flex items-start gap-2">
                <span className="text-emerald-400">‚úì</span>
                <span>Keep served model names simple and URL-safe (lowercase, hyphens)</span>
              </li>
              <li className="flex items-start gap-2">
                <span className="text-emerald-400">‚úì</span>
                <span>For production, save working configs as <strong>Recipes</strong> for easy redeployment</span>
              </li>
            </ul>
          </Card>

          <Card className="p-4 bg-red-500/5 border-red-500/20 space-y-3">
            <div className="text-[10px] font-bold text-red-300 uppercase tracking-wider">‚úó Common Pitfalls</div>
            <ul className="text-[11px] text-white/70 space-y-2">
              <li className="flex items-start gap-2">
                <span className="text-red-400">‚úó</span>
                <span>Forgetting HF token for gated models (Llama, Mistral)</span>
              </li>
              <li className="flex items-start gap-2">
                <span className="text-red-400">‚úó</span>
                <span>Setting context length higher than available VRAM can handle</span>
              </li>
              <li className="flex items-start gap-2">
                <span className="text-red-400">‚úó</span>
                <span>Using vLLM with GPT-OSS models (only llama.cpp works)</span>
              </li>
              <li className="flex items-start gap-2">
                <span className="text-red-400">‚úó</span>
                <span>Missing tokenizer for GGUF deployments</span>
              </li>
              <li className="flex items-start gap-2">
                <span className="text-red-400">‚úó</span>
                <span>Wrong base directory path (container sees /models, not host path)</span>
              </li>
            </ul>
          </Card>
        </div>
      </section>

      {/* CTA */}
      <div className="flex gap-3 justify-center pt-4">
        <Link href="/models">
          <Button variant="cyan" size="sm" className="text-[10px]">
            Open Models Page ‚Üí
          </Button>
        </Link>
      </div>

      <div className="text-[9px] text-white/20 uppercase font-black tracking-[0.3em] text-center pt-4 border-t border-white/5">
        Cortex Model Addition Guide ‚Ä¢ <a href="https://www.aulendur.com" target="_blank" rel="noopener noreferrer" className="hover:text-white/40 hover:underline transition-colors">Aulendur Labs</a>
      </div>
    </section>
  );
}

// Helper Components
function StepItem({ num, children }: { num: number; children: React.ReactNode }) {
  return (
    <li className="flex gap-2">
      <span className="shrink-0 w-5 h-5 rounded-full bg-white/10 flex items-center justify-center text-[10px] font-bold text-white/60">
        {num}
      </span>
      <span className="pt-0.5">{children}</span>
    </li>
  );
}

function WalkthroughStep({ 
  step, 
  title, 
  color, 
  children 
}: { 
  step: number; 
  title: string; 
  color: string; 
  children: React.ReactNode;
}) {
  const colors: Record<string, string> = {
    blue: 'border-blue-500/30 bg-blue-500/5',
    emerald: 'border-emerald-500/30 bg-emerald-500/5',
    amber: 'border-amber-500/30 bg-amber-500/5',
    red: 'border-red-500/30 bg-red-500/5',
    purple: 'border-purple-500/30 bg-purple-500/5',
    cyan: 'border-cyan-500/30 bg-cyan-500/5',
  };
  const numColors: Record<string, string> = {
    blue: 'bg-blue-500/20 text-blue-300 border-blue-500/30',
    emerald: 'bg-emerald-500/20 text-emerald-300 border-emerald-500/30',
    amber: 'bg-amber-500/20 text-amber-300 border-amber-500/30',
    red: 'bg-red-500/20 text-red-300 border-red-500/30',
    purple: 'bg-purple-500/20 text-purple-300 border-purple-500/30',
    cyan: 'bg-cyan-500/20 text-cyan-300 border-cyan-500/30',
  };

  return (
    <div className={cn("p-3 rounded-lg border-l-4", colors[color])}>
      <div className="flex items-center gap-2 mb-2">
        <span className={cn("px-2 py-0.5 rounded text-[10px] font-bold border", numColors[color])}>
          Step {step}
        </span>
        <span className="text-[12px] font-bold text-white">{title}</span>
      </div>
      {children}
    </div>
  );
}

