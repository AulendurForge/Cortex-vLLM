# CORTEX

**Enterprise-grade self-hosted LLM inference gateway and model management platform**

CORTEX is an OpenAI-compatible gateway and admin UI for running vLLM and llama.cpp inference engines on your own infrastructure. It provides secure access control, healthâ€‘aware routing, usage metering, and a modern admin interface.

## Key Features

### ğŸš€ Inference Gateway
- OpenAI-compatible endpoints: `/v1/chat/completions`, `/v1/completions`, `/v1/embeddings`
- Multi-engine support: vLLM (GPU) and llama.cpp (CPU/GPU)
- Health checks, circuit breaking, retries, and smart routing
- Streaming responses with time-to-first-token metrics

### ğŸ” Enterprise Security
- Multi-tenant access control with organizations, users, and API keys
- IP allowlisting and rate limiting
- Scoped permissions per model or organization
- Audit logging and usage tracking

### ğŸ“Š Observability
- Prometheus metrics integration
- Per-model inference metrics (requests, tokens, latency)
- GPU utilization and memory monitoring
- System Monitor dashboard with real-time metrics

### ğŸ”§ Model Management
- Pre-start VRAM estimation and validation
- Startup diagnostics with actionable error fixes
- Model lifecycle management (start, stop, configure)
- Recipe system for configuration templates
- Offline/air-gapped deployment support

### âš™ï¸ Advanced vLLM Configuration
- Attention backend selection
- V1/V2 engine control
- Quantization (AWQ, GPTQ, FP8, INT8)
- Debug logging and trace modes
- Custom startup arguments and environment variables

## Getting Started

1. Read the **[Quickstart (Docker)](getting-started/quickstart-docker.md)** to run the stack locally
2. Follow the **[Admin Setup Guide](getting-started/admin-setup.md)** to configure your first model
3. Explore the **[Model Management](models/model-management.md)** documentation
4. Call the API via curl or SDKs using your generated API key

## Quick Links

| Category | Documentation |
|----------|---------------|
| **Getting Started** | [Quickstart (Docker)](getting-started/quickstart-docker.md) â€¢ [Configuration](getting-started/configuration.md) |
| **API** | [OpenAI-Compatible](api/openai-compatible.md) â€¢ [Admin API](api/admin-api.md) |
| **Models** | [vLLM Guide](models/vllm.md) â€¢ [llama.cpp Guide](models/llamaCPP.md) â€¢ [HuggingFace Download](models/huggingface-model-download.md) |
| **Operations** | [Deployments](operations/deployments.md) â€¢ [Offline Deployment](operations/offline-deployment.md) â€¢ [Makefile Guide](operations/makefile-guide.md) |
| **Architecture** | [System Overview](architecture/system.md) â€¢ [Backend](architecture/backend.md) â€¢ [Frontend](architecture/frontend.md) |
| **Security** | [Security Guide](security/security.md) â€¢ [Threat Model](security/threat-model.md) |
| **Contributing** | [How to Contribute](contributing/how-to-contribute.md) â€¢ [Coding Standards](contributing/coding-standards.md) |

## Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Client Applications                     â”‚
â”‚            (curl, Python SDK, Web Apps, etc.)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     CORTEX Gateway                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ â€¢ OpenAI-compatible API  â€¢ Auth & Rate Limiting         â”‚â”‚
â”‚  â”‚ â€¢ Health-aware routing   â€¢ Usage metering               â”‚â”‚
â”‚  â”‚ â€¢ Circuit breaking       â€¢ Model registry               â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â–¼                 â–¼                 â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ vLLM Model 1 â”‚ â”‚ vLLM Model 2 â”‚ â”‚llama.cpp Mod â”‚
    â”‚   (GPU)      â”‚ â”‚   (GPU)      â”‚ â”‚   (CPU)      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## License and Ownership

Copyright Â© 2024-2025 Aulendur Labs. Licensed under the terms in `LICENSE.txt`. See `NOTICE.txt` for attributions.
